{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔖 Research Article Recommendation:\n",
    "\n",
    "**Discovering Relevant Research Articles Using Text Similarity**\n",
    "**Author: Tammireddy Sri Vallabh**\n",
    "\n",
    "---\n",
    "\n",
    "### Motivation – Why did you pick this topic?\n",
    "\n",
    "As a student constantly exploring new research areas, I often found it difficult to discover relevant academic articles quickly. Most platforms offer keyword-based search, but that often misses context or nuanced connections between research topics. I wanted to create a smarter way to surface articles based on meaning and similarity, not just keywords. This led me to build a content-based recommendation system that understands and processes natural language input to find the most relevant research articles using NLP techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "From this project, I gained hands-on experience with:\n",
    "\n",
    "- **Web scraping** using BeautifulSoup to collect academic article metadata from Springer.\n",
    "- **Natural Language Processing (NLP)** techniques like tokenization, POS tagging, and stopword removal.\n",
    "- Building **TF-IDF-based vectorizers** to convert textual data into numerical formats for similarity calculations.\n",
    "- Using **cosine similarity** to rank results based on relevance.\n",
    "- Implementing an **end-to-end Streamlit web app** with user input validation, real-time recommendations, and clean UI.\n",
    "- Understanding how multiple textual signals (titles, keywords, authors) can be combined for better document representation.\n",
    "\n",
    "---\n",
    "\n",
    "### Code / Notebook Highlights\n",
    "\n",
    "- **Data Collection**: Articles scraped from Springer journals using BeautifulSoup.\n",
    "- **Preprocessing**: Custom functions to clean and normalize text, group articles by journals, and extract useful tags.\n",
    "- **TF-IDF Matrix Generation**: Journal-level and article-level TF-IDF matrices created to support search.\n",
    "- **Web App**: Built using Streamlit with an interactive UI for topic input and link-based output.\n",
    "\n",
    "You can view the code and run the app through the `app.py` script. All models and matrices are serialized using `pickle` for easy deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflections\n",
    "\n",
    "**(a) What surprised me?**\n",
    "I was surprised at how effective a basic TF-IDF model could be when paired with thoughtful preprocessing. By cleaning and combining different text fields, the model could make surprisingly good recommendations — even without using deep learning.\n",
    "\n",
    "**(b) Scope for improvement:**\n",
    "\n",
    "- Integrate **contextual embeddings** (like Sentence-BERT) instead of TF-IDF for deeper semantic understanding.\n",
    "- Expand to **multimodal inputs** by including images, abstracts, or full papers.\n",
    "- Add **feedback mechanisms** for users to rate articles and fine-tune recommendations over time.\n",
    "- Scale to a larger dataset and optimize processing with multiprocessing or distributed computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 From Raw Data to Search-Ready Insights: How Journals Are Preprocessed\n",
    "\n",
    "When building a smart research discovery system, raw data alone isn’t enough. Behind the scenes, there's a thoughtful transformation that prepares this data for meaningful search and recommendation. Here's how it works.\n",
    "\n",
    "Imagine a huge collection of research articles, each tied to different journals, authors, and topics. To make sense of it all, we start by **organizing the articles journal by journal**. This means pulling together all article titles that belong to the same publication. Along with this, we collect all the authors who’ve contributed to these articles, and the keywords that hint at what each journal is about.\n",
    "\n",
    "Next comes the cleaning phase. Raw titles and author names often include extra punctuation, inconsistent casing, or irrelevant words. So each piece of text — whether it's an article title, an author name, or a keyword — goes through a process that removes unnecessary clutter. This ensures we're left with clean, relevant terms that truly describe the journal’s focus.\n",
    "\n",
    "Once the text is cleaned, everything is **merged into a single, rich description for each journal**. This description captures the essence of what that journal publishes, who contributes to it, and what topics it frequently covers. We call this the journal’s \"tag profile.\"\n",
    "\n",
    "This tag profile is the **backbone of the recommendation system**. When a user searches for a topic, these tags help the system quickly match the user’s interest to the most relevant journals — making sure suggestions are based on actual content, not just titles or keywords.\n",
    "\n",
    "In short, this step transforms messy, scattered metadata into an intelligent foundation for search — helping users discover journals that matter to them, faster and more accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Primary Imports\n",
    "\n",
    "The following libraries are essential for data preprocessing, natural language processing (NLP), and text similarity computation:\n",
    "\n",
    "- **os, re, unicodedata**: For basic system operations and text normalization.\n",
    "- **pandas**: For structured data manipulation.\n",
    "- **nltk**: For tokenization, part-of-speech tagging, and stopword handling.\n",
    "- **sklearn**: For TF-IDF vectorization and cosine similarity to match user input with articles.\n",
    "- **pickle**: To load pre-saved models and data efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-07T19:30:44.801430Z",
     "iopub.status.busy": "2025-05-07T19:30:44.800557Z",
     "iopub.status.idle": "2025-05-07T19:30:46.798942Z",
     "shell.execute_reply": "2025-05-07T19:30:46.798164Z",
     "shell.execute_reply.started": "2025-05-07T19:30:44.801397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These NLTK resources provide tokenization (`punkt`), part-of-speech tagging (`averaged_perceptron_tagger`), stopword filtering (`stopwords`), and internal optimizations (`punkt_tab`, `averaged_perceptron_tagger_eng`) essential for core NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:30:53.395848Z",
     "iopub.status.busy": "2025-05-07T19:30:53.395446Z",
     "iopub.status.idle": "2025-05-07T19:30:53.747187Z",
     "shell.execute_reply": "2025-05-07T19:30:53.746219Z",
     "shell.execute_reply.started": "2025-05-07T19:30:53.395826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mssri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mssri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mssri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mssri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\mssri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_paragraph` function takes a row from a DataFrame and a column name (`index`) and **Concatenates** all strings in that column (which is expected to be a list of strings) into one lowercase paragraph-like string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph(row, index):\n",
    "    ans = ''\n",
    "    for x in row[index]:\n",
    "        ans = ans + ' ' + x.lower()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `remove_accents(text)` Removes diacritical marks (accents) from characters using Unicode normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_accents(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ASCII', 'ignore').decode('utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_clean_text(row, index)`\n",
    "\n",
    "- Cleans and tokenizes text from a specified column in a row.\n",
    "- Applies the following steps:\n",
    "  - Lowercases the text\n",
    "  - Tokenizes it\n",
    "  - Removes commas and accents\n",
    "  - Filters out:\n",
    "    - Non-alphabetic tokens\n",
    "    - Stopwords (common words like “the”, “is”)\n",
    "    - Very short words\n",
    "    - Words with a dot as their second character (likely abbreviations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(row, index):\n",
    "    if not isinstance(row[index], str):\n",
    "        return ''\n",
    "    if row[index] == \"NULL\":\n",
    "        return ''\n",
    "    clean_text = ''\n",
    "    words = word_tokenize(row[index].lower())\n",
    "    for word in words:\n",
    "        word = word.replace(',', ' ')\n",
    "        word = remove_accents(word)\n",
    "        if re.match(r'^[a-zA-Z]+$', word) and word not in stop_words and len(word) > 1 and word[1] != '.':\n",
    "            clean_text += ' ' + word\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `combine(row, indices)` concatenates the text from multiple columns (`indices`) of a row into one string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(row, indices):\n",
    "    ans = ''\n",
    "    for i in indices:\n",
    "        ans = ans + ' ' + row[i]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stop_words` are a set of common English stopwords used to filter out uninformative words during cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:31:00.735697Z",
     "iopub.status.busy": "2025-05-07T19:31:00.735422Z",
     "iopub.status.idle": "2025-05-07T19:31:00.741780Z",
     "shell.execute_reply": "2025-05-07T19:31:00.740729Z",
     "shell.execute_reply.started": "2025-05-07T19:31:00.735678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('compressed_data.bz2', compression='bz2')\n",
    "main_df = main_df.drop(['item_doi'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:31:10.495769Z",
     "iopub.status.busy": "2025-05-07T19:31:10.495491Z",
     "iopub.status.idle": "2025-05-07T19:31:10.519978Z",
     "shell.execute_reply": "2025-05-07T19:31:10.518836Z",
     "shell.execute_reply.started": "2025-05-07T19:31:10.495742Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_title</th>\n",
       "      <th>publication_title</th>\n",
       "      <th>authors</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>url</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correction to strong field physics pursued wit...</td>\n",
       "      <td>aapps bulletin</td>\n",
       "      <td>vishwa bandhu pathakseong ku leeki hong paecal...</td>\n",
       "      <td>2021</td>\n",
       "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
       "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>correction time reversal and reciprocity</td>\n",
       "      <td>aapps bulletin</td>\n",
       "      <td>olivier sigwarthchristian miniatura</td>\n",
       "      <td>2022</td>\n",
       "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
       "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ultrasound detection using microcavity raman l...</td>\n",
       "      <td>aapps bulletin</td>\n",
       "      <td>xiao</td>\n",
       "      <td>2022</td>\n",
       "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
       "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relativistic density functional theory in nucl...</td>\n",
       "      <td>aapps bulletin</td>\n",
       "      <td>jie mengpengwei zhao</td>\n",
       "      <td>2021</td>\n",
       "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
       "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muon cooling and acceleration</td>\n",
       "      <td>aapps bulletin</td>\n",
       "      <td>masashi otani</td>\n",
       "      <td>2022</td>\n",
       "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
       "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          item_title publication_title  \\\n",
       "0  correction to strong field physics pursued wit...    aapps bulletin   \n",
       "1           correction time reversal and reciprocity    aapps bulletin   \n",
       "2  ultrasound detection using microcavity raman l...    aapps bulletin   \n",
       "3  relativistic density functional theory in nucl...    aapps bulletin   \n",
       "4                      muon cooling and acceleration    aapps bulletin   \n",
       "\n",
       "                                             authors  publication_year  \\\n",
       "0  vishwa bandhu pathakseong ku leeki hong paecal...              2021   \n",
       "1                olivier sigwarthchristian miniatura              2022   \n",
       "2                                               xiao              2022   \n",
       "3                               jie mengpengwei zhao              2021   \n",
       "4                                      masashi otani              2022   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://link.springer.com/article/10.1007/s4367...   \n",
       "1  http://link.springer.com/article/10.1007/s4367...   \n",
       "2  http://link.springer.com/article/10.1007/s4367...   \n",
       "3  http://link.springer.com/article/10.1007/s4367...   \n",
       "4  http://link.springer.com/article/10.1007/s4367...   \n",
       "\n",
       "                                            keywords  \n",
       "0  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
       "1  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
       "2  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
       "3  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
       "4  AAPPS Bulletin Atomic Molecular Optical and Pl...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below function does Preprocessing and Tag Generation for Journals\n",
    "\n",
    "This function takes a raw DataFrame of article metadata and processes it to generate a cleaned and structured DataFrame at the **journal level**, with enriched text features for similarity-based search or NLP tasks.\n",
    "\n",
    "#### 🔧 Key Processing Steps:\n",
    "\n",
    "1. **Group Articles by Journal**\n",
    "\n",
    "2. **Group Authors by Journal**\n",
    "\n",
    "3. **Extract Keywords for Each Journal**\n",
    "\n",
    "4. **Join the Article Titles, Authors, and Keywords**\n",
    "\n",
    "5. **Text Cleaning**\n",
    "\n",
    "6. **Create Combined Tags**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:31:13.501709Z",
     "iopub.status.busy": "2025-05-07T19:31:13.500867Z",
     "iopub.status.idle": "2025-05-07T19:31:13.509367Z",
     "shell.execute_reply": "2025-05-07T19:31:13.508389Z",
     "shell.execute_reply.started": "2025-05-07T19:31:13.501679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_journal_df(df):\n",
    "    journal_art = df.groupby('publication_title')['item_title'].apply(\n",
    "        list).reset_index(name='Articles')\n",
    "    journal_art.set_index(['publication_title'], inplace=True)\n",
    "\n",
    "    journal_auth = df.groupby('publication_title')['authors'].apply(\n",
    "        list).reset_index(name='authors')\n",
    "    journal_auth.set_index('publication_title', inplace=True)\n",
    "\n",
    "    journal_key = df.drop_duplicates(\n",
    "        subset=[\"publication_title\", \"keywords\"], keep='first')\n",
    "    journal_key = journal_key.drop(\n",
    "        ['item_title', 'authors', 'publication_year', 'url'], axis=1)\n",
    "    journal_key.set_index(['publication_title'], inplace=True)\n",
    "\n",
    "    journal_main = journal_art.join([journal_key, journal_auth])\n",
    "    journal_main.reset_index(inplace=True)\n",
    "\n",
    "    journal_main['Articles'] = journal_main.apply(\n",
    "        get_paragraph, index='Articles', axis=1)\n",
    "    journal_main['Articles'] = journal_main.apply(\n",
    "        get_clean_text, index='Articles', axis=1)\n",
    "    journal_main['authors'] = journal_main.apply(\n",
    "        get_paragraph, index='authors', axis=1)\n",
    "    journal_main['authors'] = journal_main.apply(\n",
    "        get_clean_text, index='authors', axis=1)\n",
    "    journal_main['keywords'] = journal_main.apply(\n",
    "        get_clean_text, index='keywords', axis=1)\n",
    "\n",
    "    journal_main['Tags'] = journal_main.apply(\n",
    "        combine, indices=['keywords', 'Articles', 'authors'], axis=1)\n",
    "    journal_main['Tags'] = journal_main.apply(\n",
    "        get_clean_text, index='Tags', axis=1)\n",
    "\n",
    "    return journal_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying it to journal_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:31:15.935749Z",
     "iopub.status.busy": "2025-05-07T19:31:15.935451Z",
     "iopub.status.idle": "2025-05-07T19:32:09.025223Z",
     "shell.execute_reply": "2025-05-07T19:32:09.024304Z",
     "shell.execute_reply.started": "2025-05-07T19:31:15.935727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journal_main processed\n"
     ]
    }
   ],
   "source": [
    "# Journal Dataframe\n",
    "journal_main = get_journal_df(main_df)\n",
    "print('journal_main processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:57:51.690585Z",
     "iopub.status.busy": "2025-05-07T19:57:51.690275Z",
     "iopub.status.idle": "2025-05-07T19:57:54.315539Z",
     "shell.execute_reply": "2025-05-07T19:57:54.314702Z",
     "shell.execute_reply.started": "2025-05-07T19:57:51.690562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved journal_main.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save journal_main\n",
    "with open(\"journal_main.pkl\", \"wb\") as f:\n",
    "    pickle.dump(journal_main, f)\n",
    "\n",
    "print(\"Saved journal_main.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "#### Formula Explanation\n",
    "\n",
    "1. **Term Frequency (TF)**:\n",
    "   The formula for **TF** is:\n",
    "\n",
    "   $$\n",
    "   \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "   $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   The formula for **IDF** is:\n",
    "\n",
    "   $$\n",
    "   \\text{IDF}(t) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing the term } t} \\right)\n",
    "   $$\n",
    "\n",
    "3. **TF-IDF Calculation**:\n",
    "   The TF-IDF score of a term \\( t \\) in a document \\( d \\) is calculated as the product of the term's TF and IDF scores:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:32:20.255840Z",
     "iopub.status.busy": "2025-05-07T19:32:20.255539Z",
     "iopub.status.idle": "2025-05-07T19:32:20.261301Z",
     "shell.execute_reply": "2025-05-07T19:32:20.260441Z",
     "shell.execute_reply.started": "2025-05-07T19:32:20.255817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tfidfs(journal_main):\n",
    "    vectorizer = TfidfVectorizer(decode_error='ignore', strip_accents='ascii')\n",
    "    journal_tfidf_matrix = vectorizer.fit_transform(journal_main['Tags'])\n",
    "    return vectorizer, journal_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:32:23.311505Z",
     "iopub.status.busy": "2025-05-07T19:32:23.310819Z",
     "iopub.status.idle": "2025-05-07T19:32:31.638685Z",
     "shell.execute_reply": "2025-05-07T19:32:31.637760Z",
     "shell.execute_reply.started": "2025-05-07T19:32:23.311475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfids and vectorizer for journals completed\n"
     ]
    }
   ],
   "source": [
    "vectorizer, journal_tfidf_matrix = get_tfidfs(journal_main)\n",
    "print('tfids and vectorizer for journals completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:56:23.045638Z",
     "iopub.status.busy": "2025-05-07T19:56:23.045326Z",
     "iopub.status.idle": "2025-05-07T19:56:23.654919Z",
     "shell.execute_reply": "2025-05-07T19:56:23.654008Z",
     "shell.execute_reply.started": "2025-05-07T19:56:23.045615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vectorizer.pkl and journal_tfidf_matrix.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the vectorizer\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save the journal TF-IDF matrix\n",
    "with open(\"journal_tfidf_matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(journal_tfidf_matrix, f)\n",
    "\n",
    "print(\"Saved vectorizer.pkl and journal_tfidf_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_article_df(row)` processes and transforms article data by cleaning and extracting relevant information from a DataFrame containing journal articles. Below is a step-by-step explanation of each part of the code:\n",
    "\n",
    "### 1. **Filter Data to Select the Right Article**\n",
    "\n",
    "### 2. **Clean Text Data for Item Title and Authors**\n",
    "\n",
    "### 3. **Tokenize the Item Title**\n",
    "\n",
    "### 4. **Apply Part-of-Speech (POS) Tagging**\n",
    "\n",
    "### 5. **Extract Nouns and Adjectives**\n",
    "\n",
    "### 6. **Further Processing on Extracted Tags**\n",
    "\n",
    "### 7. **Add Authors and Publication Year to Tags**\n",
    "\n",
    "### 8. **Drop Unnecessary Columns**\n",
    "\n",
    "### 9. **Reset and Set Index**\n",
    "\n",
    "### 10. **Return the Processed DataFrame**\n",
    "\n",
    "This approach allows you to extract meaningful information from the text of journal articles, making it easier to analyze or build features for tasks like text mining or natural language processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:32:39.696684Z",
     "iopub.status.busy": "2025-05-07T19:32:39.696401Z",
     "iopub.status.idle": "2025-05-07T19:32:39.707430Z",
     "shell.execute_reply": "2025-05-07T19:32:39.706329Z",
     "shell.execute_reply.started": "2025-05-07T19:32:39.696665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_article_df(row):\n",
    "    article = main_df.loc[main_df['publication_title'] ==\n",
    "                          journal_main['publication_title'][row.name]].copy()\n",
    "    article['item_title'] = article.apply(\n",
    "        get_clean_text, index='item_title', axis=1)\n",
    "    article['authors'] = article.apply(get_clean_text, index='authors', axis=1)\n",
    "    article['Tokenized'] = article['item_title'].apply(word_tokenize)\n",
    "    article['Tagged'] = article['Tokenized'].apply(pos_tag)\n",
    "    article['Tags'] = article['Tagged'].apply(lambda x: [word for word, tag in x if\n",
    "                                                         tag.startswith('NN') or tag.startswith('JJ') and word.lower() not in stop_words])\n",
    "    article['Tags'] = article.apply(get_paragraph, index='Tags', axis=1)\n",
    "    article['Tags'] = article.apply(\n",
    "        lambda x: x['Tags'] + ' ' + x['authors'] + ' ' + str(x['publication_year']), axis=1)\n",
    "    article = article.drop(['keywords', 'publication_title',\n",
    "                           'Tokenized', 'Tagged', 'authors', 'publication_year'], axis=1)\n",
    "    article.reset_index(inplace=True)\n",
    "    article.set_index('index', inplace=True)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_vectorizer` function initializes and returns a `TfidfVectorizer` with options to ignore decoding errors and strip accents to ASCII characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(row):\n",
    "    vectorizer = TfidfVectorizer(decode_error='ignore', strip_accents='ascii')\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fit a TF-IDF vectorizer to the 'Tags' column of the `article_df` DataFrame and returns the resulting TF-IDF matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_matrix(row):\n",
    "    tfidf_matrix = row['article_vectorizer'].fit_transform(\n",
    "        row['article_df']['Tags'])\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `article_preprocessing` function processes a DataFrame `df` by:\n",
    "\n",
    "1. Applying `get_article_df` to each row to extract and clean article data, storing the result in the `article_df` column.\n",
    "2. Applying `get_vectorizer` to each row to create a TF-IDF vectorizer for the article text, storing it in the `article_vectorizer` column.\n",
    "3. Applying `get_tfidf_matrix` to each row to generate a TF-IDF matrix using the vectorizer, storing it in the `article_matrix` column.\n",
    "\n",
    "It returns the updated DataFrame with these new columns for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_preprocessing(df):\n",
    "    df['article_df'] = df.apply(get_article_df, axis=1)\n",
    "    df['article_vectorizer'] = df.apply(get_vectorizer, axis=1)\n",
    "    df['article_matrix'] = df.apply(get_tfidf_matrix, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:36:37.947884Z",
     "iopub.status.busy": "2025-05-07T19:36:37.947568Z",
     "iopub.status.idle": "2025-05-07T19:41:31.879664Z",
     "shell.execute_reply": "2025-05-07T19:41:31.878881Z",
     "shell.execute_reply.started": "2025-05-07T19:36:37.947860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "journal_main = article_preprocessing(journal_main)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved journal_main.pkl\n"
     ]
    }
   ],
   "source": [
    "journal_main.to_pickle('journal_main.pkl')\n",
    "print('saved journal_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:41:57.332267Z",
     "iopub.status.busy": "2025-05-07T19:41:57.331952Z",
     "iopub.status.idle": "2025-05-07T19:42:04.068312Z",
     "shell.execute_reply": "2025-05-07T19:42:04.067190Z",
     "shell.execute_reply.started": "2025-05-07T19:41:57.332246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit pyngrok --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Here’s an explanation **before** the code, introducing each function and its purpose—ideal for documentation, a blog, or your notebook:\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Building Streamlit App\n",
    "\n",
    "This app recommends **research articles** based on a user’s input (topic, phrase, or keyword). It uses Natural Language Processing (NLP), specifically **TF-IDF** and **cosine similarity**, to find and rank relevant articles from a dataset of journal publications. Below, we explain the **purpose of each function** used in the app, **before showing the code**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 `get_journal_index(user_input)`\n",
    "\n",
    "This function identifies the most relevant journals based on a user's query:\n",
    "\n",
    "- It transforms the input into a TF-IDF vector.\n",
    "- It then compares the vector with precomputed TF-IDF representations of journal content.\n",
    "- Using **cosine similarity**, it finds and returns the indices of the top-matching journals.\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 `get_article_recommendations(user_input)`\n",
    "\n",
    "Once we have the top journals, this function digs deeper:\n",
    "\n",
    "- For each recommended journal, it uses that journal’s own vectorizer to encode the input.\n",
    "- It then compares this vector with TF-IDF vectors of the articles in that journal.\n",
    "- The most similar articles are collected and returned as recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 `get_links(user_input)`\n",
    "\n",
    "This function brings everything together:\n",
    "\n",
    "- First, it **validates** the input using `validation()`.\n",
    "- If the input is valid, it calls `get_article_recommendations()` to get article matches.\n",
    "- Then it extracts useful metadata like title, link, article ID, and journal ID for each result.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ `validation(text)`\n",
    "\n",
    "Before doing any analysis, we want to make sure the input is meaningful:\n",
    "\n",
    "- This function uses **Part-of-Speech tagging** to extract adjectives and nouns from the input.\n",
    "- If the input lacks relevant words, it marks it as invalid.\n",
    "- Otherwise, it returns a cleaned version of the input to be used in search.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 The Streamlit User Interface\n",
    "\n",
    "The rest of the code builds the user interface:\n",
    "\n",
    "- It loads preprocessed data and vectorizers from `.pkl` files.\n",
    "- It provides a search box where users can enter topics.\n",
    "- On submit, it shows a list of matching articles with clickable links.\n",
    "- It also includes warning messages if input is too vague.\n",
    "\n",
    "---\n",
    "\n",
    "Once these functions are in place, the user can simply **enter a topic** like “AI in medicine” and instantly discover relevant academic articles sorted by relevance.\n",
    "\n",
    "Would you like me to format this as a markdown cell for your notebook?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:21:48.952882Z",
     "iopub.status.busy": "2025-05-07T20:21:48.951567Z",
     "iopub.status.idle": "2025-05-07T20:21:48.966836Z",
     "shell.execute_reply": "2025-05-07T20:21:48.965943Z",
     "shell.execute_reply.started": "2025-05-07T20:21:48.952838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load models and data\n",
    "with open(\"journal_main.pkl\", \"rb\") as f:\n",
    "    journal_main = pickle.load(f)\n",
    "\n",
    "with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open(\"journal_tfidf_matrix.pkl\", \"rb\") as f:\n",
    "    journal_tfidf_matrix = pickle.load(f)\n",
    "\n",
    "# Parameters\n",
    "journal_threshold = 4\n",
    "article_threshold = 10\n",
    "\n",
    "# Logic\n",
    "def get_journal_index(user_input):\n",
    "    user_tfidf = vectorizer.transform([user_input])\n",
    "    cosine_similarities = cosine_similarity(user_tfidf, journal_tfidf_matrix).flatten()\n",
    "    indices = cosine_similarities.argsort()[::-1]\n",
    "    top_recommendations = [i for i in indices if cosine_similarities[i] > 0][:min(journal_threshold, len(indices))]\n",
    "    return top_recommendations\n",
    "\n",
    "def get_article_recommendations(user_input):\n",
    "    recommended_journals = get_journal_index(user_input)\n",
    "    recommendations = []\n",
    "    for journal_id in recommended_journals:\n",
    "        user_tfidf = journal_main['article_vectorizer'][journal_id].transform([user_input])\n",
    "        cosine_similarities = cosine_similarity(user_tfidf, journal_main['article_matrix'][journal_id]).flatten()\n",
    "        indices = cosine_similarities.argsort()[::-1]\n",
    "        top_recommendation_articles = [\n",
    "            (cosine_similarities[i], i, journal_id)\n",
    "            for i in indices if cosine_similarities[i] > 0\n",
    "        ][:min(article_threshold, len(indices))]\n",
    "        recommendations += top_recommendation_articles\n",
    "    recommendations.sort(reverse=True)\n",
    "    return recommendations\n",
    "\n",
    "def get_links(user_input):\n",
    "    check = validation(user_input)\n",
    "    if check['validation'] == 'valid':\n",
    "        recommendations = get_article_recommendations(check['sentence'])\n",
    "        links = []\n",
    "        for article in recommendations:\n",
    "            similarity, article_id, journal_id = article\n",
    "            link = {\n",
    "                \"title\": journal_main['article_df'][journal_id].iloc[article_id, 0],\n",
    "                \"url\": journal_main['article_df'][journal_id].iloc[article_id, 1],\n",
    "                \"article_id\": int(article_id),\n",
    "                \"journal_id\": int(journal_id)\n",
    "            }\n",
    "            links.append(link)\n",
    "        return links\n",
    "    return []\n",
    "\n",
    "def validation(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    adjectives = [word for word, pos in tagged_words if pos.startswith('JJ')]\n",
    "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
    "\n",
    "    result = {}\n",
    "    if not adjectives and not nouns:\n",
    "        result['validation'] = 'invalid'\n",
    "    else:\n",
    "        combined_sentence = f\"{' '.join(adjectives)} {' '.join(nouns)}\".strip()\n",
    "        result['validation'] = 'valid'\n",
    "        result['sentence'] = combined_sentence\n",
    "\n",
    "    return result\n",
    "\n",
    "# Streamlit UI\n",
    "st.set_page_config(page_title=\"Discover Research Articles\", layout=\"centered\")\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "        .title { text-align: center; font-size: 36px; color: #2c3e50; font-weight: bold; margin-top: 30px; }\n",
    "        .subtitle { text-align: center; font-size: 18px; color: #333; margin-bottom: 20px; font-style: italic; }\n",
    "        .section { padding: 30px; margin-bottom: 40px; }\n",
    "        .input-box { width: 100%; padding: 15px; font-size: 18px; border-radius: 8px; border: 1px solid #ddd; background-color: #ffffff; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n",
    "        .button { background-color: #3498db; color: white; padding: 15px 25px; font-size: 20px; border: none; border-radius: 8px; cursor: pointer; width: 100%; }\n",
    "        .button:hover { background-color: #2980b9; }\n",
    "        .article { padding: 15px; margin-bottom: 12px; border-radius: 8px; background-color: #ffffff; box-shadow: 0 3px 6px rgba(0,0,0,0.1); }\n",
    "        .article-title { font-size: 20px; color: #3498db; font-weight: bold; text-decoration: none; }\n",
    "        .article-meta { font-size: 12px; color: #888; margin-top: 5px; }\n",
    "    </style>\n",
    "    <div class=\"title\">🔎 Discover Relevant Research Articles</div>\n",
    "    <div class=\"subtitle\">Enter a topic, keyword, or phrase to explore the latest articles in your field of interest!</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "with st.container():\n",
    "    st.markdown('<div class=\"section\">', unsafe_allow_html=True)\n",
    "    st.subheader(\"Find Research Articles\")\n",
    "    st.markdown(\"Type in a topic or keyword that you'd like to explore. Our system will fetch articles based on your input.\")\n",
    "    article_input = st.text_input(\"\", placeholder=\"e.g., Quantum Computing, AI, Rocket Science ...\", key=\"article_input\", \n",
    "                                  help=\"Enter your research topic or keyword to find related articles.\")\n",
    "    \n",
    "    if st.button(\"🔗 Find Articles\", key=\"generate_links\"):\n",
    "        if article_input:\n",
    "            validation_result = validation(article_input)\n",
    "            \n",
    "            if validation_result['validation'] == 'invalid':\n",
    "                st.warning(\"Please try entering more descriptive terms, including nouns or adjectives.\")\n",
    "            else:\n",
    "                result = get_links(validation_result['sentence'])\n",
    "                \n",
    "                if result:\n",
    "                    st.markdown(\"### 🔗 Top Matching Articles\", unsafe_allow_html=True)\n",
    "                    for i, article in enumerate(result):\n",
    "                        st.markdown(f\"\"\"\n",
    "                            <div class=\"article\">\n",
    "                                <a class=\"article-title\" href=\"{article['url']}\" target=\"_blank\">\n",
    "                                    {i+1}. {article['title']}\n",
    "                                </a>\n",
    "                                <p class=\"article-meta\">Article ID: {article['article_id']} | Journal ID: {article['journal_id']}</p>\n",
    "                            </div>\n",
    "                        \"\"\", unsafe_allow_html=True)\n",
    "                else:\n",
    "                    st.warning(\"No articles matched your query. Try being more specific with your input.\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a topic or keyword to get article recommendations.\")\n",
    "    \n",
    "    st.markdown('</div>', unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T19:42:17.616821Z",
     "iopub.status.busy": "2025-05-07T19:42:17.616506Z",
     "iopub.status.idle": "2025-05-07T19:42:19.004721Z",
     "shell.execute_reply": "2025-05-07T19:42:19.003981Z",
     "shell.execute_reply.started": "2025-05-07T19:42:17.616798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Set your authtoken (replace with your real one)\n",
    "ngrok.set_auth_token(\"2wm7thIz3K5BCEVCmPBP3Ka6mB8_46uj7zKLMdGV3bykSfT4W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T20:21:52.396492Z",
     "iopub.status.busy": "2025-05-07T20:21:52.396206Z",
     "iopub.status.idle": "2025-05-07T20:21:57.915428Z",
     "shell.execute_reply": "2025-05-07T20:21:57.914377Z",
     "shell.execute_reply.started": "2025-05-07T20:21:52.396473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Your app is live at: NgrokTunnel: \"https://44d1-37-120-216-226.ngrok-free.app\" -> \"http://localhost:8502\"\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "ngrok.kill()\n",
    "# Start Streamlit in a separate thread\n",
    "def run():\n",
    "    !streamlit run app.py\n",
    "\n",
    "thread = threading.Thread(target=run)\n",
    "thread.start()\n",
    "\n",
    "# Wait for it to boot up\n",
    "time.sleep(5)\n",
    "\n",
    "# Connect ngrok properly\n",
    "public_url = ngrok.connect(8502, \"http\", bind_tls=True)\n",
    "print(f\"🌐 Your app is live at: {public_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7357626,
     "sourceId": 11720638,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
